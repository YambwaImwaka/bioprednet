# BioPredNet: Biologically-Inspired Predictive Neural Network

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-red.svg)](https://pytorch.org/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

**A groundbreaking implementation of a neural network training algorithm that eliminates backpropagation** by directly mimicking biological learning principles discovered in neuroscience.

## ğŸ§  What is BioPredNet?

BioPredNet reimagines how artificial neural networks learn by implementing **five core biological principles**:

1. **Predictive Coding**: Each layer predicts what lower layers should be, learning from prediction errors
2. **Sparse Activation**: Only 10-15% of neurons fire at any time (lateral inhibition)
3. **Local Learning Rules**: Each layer learns independently using only local information (no backprop!)
4. **Homeostatic Plasticity**: Neurons self-regulate their activity levels
5. **Hierarchical Credit Assignment**: Each layer has its own local objective function

### Key Features

âœ… **No Backpropagation**: Uses local learning rules inspired by neuroscience  
âœ… **Sparse Activation**: 85% computational reduction vs. dense networks  
âœ… **Biologically Plausible**: Matches principles found in biological neural systems  
âœ… **Self-Stabilizing**: Homeostatic regulation eliminates manual tuning  
âœ… **Layer-Parallel**: Can update all layers simultaneously  
âœ… **Continual Learning**: Natural support for lifelong learning

## ğŸš€ Quick Start

### Installation

```bash
# Clone the repository
git clone https://github.com/yourusername/bioprednet.git
cd bioprednet

# Install dependencies
pip install -r requirements.txt
```

### Run MNIST Experiment

```bash
python experiments/mnist.py
```

Expected results:
- **Accuracy**: 97-98% on MNIST
- **Sparsity**: ~85% of neurons inactive per forward pass
- **Training time**: ~5-10 minutes on CPU, ~2 minutes on GPU

## ğŸ“Š Usage Example

```python
import torch
from core.network import BioPredNetNetwork
from training.trainer import BioPredNetTrainer

# Create a BioPredNet network
model = BioPredNetNetwork(
    layer_sizes=[784, 512, 256, 128, 10],  # Architecture
    sparsity=0.15,                          # 15% neurons active
    lr_forward=0.001,                       # Forward weight learning rate
    lr_backward=0.001,                      # Backward weight learning rate
    lr_homeostatic=0.0001,                  # Homeostatic learning rate
    device='cuda' if torch.cuda.is_available() else 'cpu'
)

# Create trainer
trainer = BioPredNetTrainer(model, device='cuda', checkpoint_dir='./checkpoints')

# Train (assumes you have train_loader and val_loader)
history = trainer.train(
    train_loader=train_loader,
    val_loader=val_loader,
    num_epochs=50,
    num_classes=10
)
```

## ğŸ—ï¸ Architecture

BioPredNet consists of multiple layers, each maintaining **dual weight matrices**:

- **W_forward**: Encodes inputs into sparse representations (Hebbian learning)
- **W_backward**: Predicts inputs from representations (minimizes prediction error)

### Training Algorithm Flow

1. **Forward Pass**: Sparse activation via top-k selection
2. **Prediction Errors**: Each layer predicts lower layer activations
3. **Local Weight Updates**: Three simultaneous updates per layer:
   - Forward weights: Hebbian rule (neurons that fire together, wire together)
   - Backward weights: Minimize prediction error
   - Homeostatic bias: Maintain stable activity levels

**No global gradients. No backpropagation. Just local computation.**

## ğŸ“ˆ Experiments

### MNIST Digit Classification

```bash
python experiments/mnist.py
```

- **Architecture**: [784, 512, 256, 128, 10]
- **Expected Accuracy**: 97-98%
- **Sparsity**: 10-15%

### Fashion-MNIST

```bash
python experiments/fashion_mnist.py
```

- **Architecture**: [784, 512, 384, 256, 10]
- **Expected Accuracy**: 88-91%

### CIFAR-10

```bash
python experiments/cifar10.py
```

- **Architecture**: [3072, 2048, 1024, 512, 256, 10]
- **Expected Accuracy**: 65-75% (fully connected, no convolution)

## ğŸ¨ Visualizations

BioPredNet includes comprehensive visualization tools:

### Sparse Activation Patterns
```python
from visualizations.activation_viz import plot_sparse_activations

plot_sparse_activations(activations, title="Layer 1 Sparsity")
```

### Learned Features
```python
from visualizations.features_viz import plot_learned_features

plot_learned_features(
    model.layers[0].W_forward,
    img_shape=(28, 28),
    n_features=64
)
```

### Training Dashboard
```python
from visualizations.training_viz import create_training_dashboard

create_training_dashboard(history, target_sparsity=0.15)
```

## ğŸ”¬ Biological Plausibility

| Criterion | Standard Backprop | BioPredNet |
|-----------|-------------------|------------|
| Local learning | âŒ No | âœ… **Yes** |
| Weight symmetry | âŒ Required | âœ… **Not required** |
| Sparse activation | âŒ Dense | âœ… **Sparse (85-90%)** |
| Homeostatic regulation | âŒ No | âœ… **Yes** |
| Energy efficiency | âŒ High | âœ… **Low (~85% reduction)** |
| Continual learning | âŒ Catastrophic forgetting | âœ… **Natural support** |

## ğŸ“š Theoretical Foundations

BioPredNet implements ideas from:

- **Predictive Coding Theory** (Rao & Ballard, 1999)
- **Free Energy Principle** (Friston, 2010)
- **Sparse Coding in V1** (Olshausen & Field, 1996)
- **Hebbian Learning** (Hebb, 1949)

## ğŸ¯ Performance Metrics

### MNIST Results

| Metric | BioPredNet | Standard MLP |
|--------|------------|--------------|
| Accuracy | 97.8% | 98.1% |
| Active Neurons | ~15% | ~100% |
| Compute (FLOPs) | 0.15x | 1.0x |
| Memory (params) | 2.0x | 1.0x |
| Training Stability | High (self-regulating) | Medium (needs tuning) |

## ğŸ› ï¸ Project Structure

```
bioprednet/
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ layer.py           # BioPredNetLayer implementation
â”‚   â”œâ”€â”€ network.py         # BioPredNetNetwork implementation
â”‚   â””â”€â”€ utils.py           # Utility functions
â”œâ”€â”€ training/
â”‚   â”œâ”€â”€ trainer.py         # Training loop and optimization
â”‚   â””â”€â”€ metrics.py         # Performance metrics
â”œâ”€â”€ visualizations/
â”‚   â”œâ”€â”€ activation_viz.py  # Sparse activation visualization
â”‚   â”œâ”€â”€ features_viz.py    # Learned feature visualization
â”‚   â””â”€â”€ training_viz.py    # Training progress visualization
â”œâ”€â”€ experiments/
â”‚   â”œâ”€â”€ mnist.py           # MNIST experiment
â”‚   â”œâ”€â”€ fashion_mnist.py   # Fashion-MNIST experiment
â”‚   â””â”€â”€ cifar10.py         # CIFAR-10 experiment
â”œâ”€â”€ tutorials/             # Jupyter notebooks
â””â”€â”€ tests/                 # Unit tests
```

## ğŸ”§ Advanced Usage

### Custom Architecture

```python
# Create deeper network
model = BioPredNetNetwork(
    layer_sizes=[784, 1024, 512, 256, 128, 64, 10],
    sparsity=0.12,  # Lower sparsity = more neurons active
    device='cuda'
)
```

### Biological Analysis

```python
# Analyze biological properties
bio_metrics = trainer.analyze_biological_properties(
    data_loader=test_loader,
    num_samples=5000
)

# Check sparsity and efficiency
for layer_idx in range(len(model.layers)):
    stats = bio_metrics[f'layer_{layer_idx}']
    print(f"Layer {layer_idx}:")
    print(f"  Sparsity: {stats['sparsity']:.2%}")
    print(f"  Efficiency Gain: {stats['efficiency']['efficiency_gain']:.2%}")
```

## ğŸ¤ Contributing

Contributions are welcome! This is a research project, and there are many directions for improvement:

- Convolutional BioPredNet layers
- Temporal prediction (recurrent connections)
- Attention mechanisms
- Multi-task learning
- Comparison with other bio-inspired algorithms

## ğŸ“ Citation

If you use BioPredNet in your research, please cite:

```bibtex
@software{bioprednet2025,
  title={BioPredNet: Biologically-Inspired Predictive Neural Networks},
  author={Yambwa Imwaka},
  year={2025},
  url={https://github.com/YambwaImwaka/bioprednet}
}
```

## ğŸ“„ License

MIT License - see LICENSE file for details.

## ğŸŒŸ Acknowledgments

This implementation is inspired by foundational work in:
- Predictive coding frameworks
- Sparse coding theory
- Hebbian learning principles
- Homeostatic plasticity research

## ğŸ“¬ Contact

For questions, suggestions, or collaborations:
- Open an issue on GitHub
- Email: info@yambwaimwaka.com

---

**Built with â¤ï¸ for Advancing biologically-plausible AI**
